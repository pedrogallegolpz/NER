{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79a89c88-00d0-42e5-a4f9-24568ce46f54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-08-19 06:16:20.891608: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-19 06:16:20.891678: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRUEBA: hablando cia cosa conozco\n"
     ]
    }
   ],
   "source": [
    "# Spacy library for NER\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.scorer import Scorer, get_ner_prf\n",
    "from spacy import displacy\n",
    "\n",
    "# NLTK for removing stopwords from corpus\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Aux lib\n",
    "from glob import glob        # For searching the dataset path\n",
    "import json                  # For reading the dataset\n",
    "import numpy as np           # util\n",
    "import pandas as pd          # util\n",
    "from tqdm import tqdm        # util\n",
    "\n",
    "\n",
    "import re                    # For removing punctuation\n",
    "\n",
    "# viz libs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import copy\n",
    "\n",
    "import pickle \n",
    "import os\n",
    "\n",
    "# FLAIR\n",
    "from flair.embeddings import WordEmbeddings, DocumentRNNEmbeddings\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "from flair.data import Corpus\n",
    "from flair.datasets import CONLL_03_SPANISH\n",
    "from flair.embeddings import WordEmbeddings, StackedEmbeddings, FlairEmbeddings\n",
    "\n",
    "\n",
    "# Google Cloud\n",
    "from google.cloud import language_v1\n",
    "\n",
    "\n",
    "\n",
    "def normalize_str(s):\n",
    "    replacements = (\n",
    "        (\"á\", \"a\"),\n",
    "        (\"é\", \"e\"),\n",
    "        (\"í\", \"i\"),\n",
    "        (\"ó\", \"o\"),\n",
    "        (\"ú\", \"u\"),\n",
    "    )\n",
    "    for a, b in replacements:\n",
    "        s = s.replace(a, b).replace(a.upper(), b.upper())\n",
    "    return s\n",
    "\n",
    "def remove_nextlines(s):\n",
    "    s = s.replace('\\r\\n','\\n').replace('-\\n','')\n",
    "    s = s.replace('\\n\\n','\\n').replace('\\n',' ')\n",
    "    return s\n",
    "\n",
    "def remove_punctuation(s, sub=' '):\n",
    "    s = re.sub(r'[^\\w\\s]', sub, s)\n",
    "    \n",
    "    for i in range(5):\n",
    "        s = s.replace('  ',' ')\n",
    "        \n",
    "    s = s.strip()\n",
    "    \n",
    "    return s \n",
    "\n",
    "def remove_stopwords(s):\n",
    "    \"\"\"\n",
    "        s is a lowercase string\n",
    "    \"\"\"\n",
    "    s = s.lower()\n",
    "    regular_expr = r'\\b(' + '|'.join(stopwords.words('spanish')) + r')\\b'\n",
    "    s = re.sub(regular_expr, '', s)\n",
    "    \n",
    "    for i in range(5):\n",
    "        s = s.replace('  ',' ')\n",
    "        \n",
    "    s = s.strip()\n",
    "    \n",
    "    return s\n",
    "\n",
    "def remove_len3(s, stop_words=[]):\n",
    "    s_split = s.split(' ')\n",
    "    s_new = []\n",
    "    for token_ in s_split:\n",
    "        token = token_.strip()\n",
    "        if len(token)>3 or token.isupper():\n",
    "            s_new.append(token)\n",
    "            \n",
    "    return ' '.join(s_new)\n",
    "\n",
    "def prepare_string(s):\n",
    "    s = remove_len3(s)\n",
    "    \n",
    "    s = s.lower()\n",
    "    \n",
    "    s = remove_stopwords(s)     # this funct has to be before normalize_str(s) funct\n",
    "    s = normalize_str(s)\n",
    "    s = remove_nextlines(s)\n",
    "    s = remove_punctuation(s)\n",
    "    \n",
    "    s = s.strip()\n",
    "    \n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "print(\"PRUEBA:\",prepare_string(\"No se de que me estás hablando si de la CIA o de otra cosa que no conozco\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64b6c2a-dc1b-4a53-b3bd-b39332693679",
   "metadata": {},
   "source": [
    "## Testeo con dataset WikiNer y CoNLL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78c2062-cd1f-432f-9afc-17ad7aae93cb",
   "metadata": {},
   "source": [
    "#### CoNLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ace47b9-82f0-4fb6-b319-2aebf5a594fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conll_pre(path='conll'):\n",
    "    with open('conll/esp.testa.txt') as f:\n",
    "        testa = f.read()\n",
    "\n",
    "    with open('conll/esp.testb.txt') as f:\n",
    "        testb = f.read()\n",
    "\n",
    "\n",
    "    conll_pre=[]\n",
    "    for t in f'{testa}\\n\\n- O\\n\\n{testb}'.split('\\n\\n- O\\n\\n'):\n",
    "        texto = t.replace('\\n\\n','_SEPARADOR_')\n",
    "        texto = '\\n'.join(texto.strip().split('_SEPARADOR_')[:-1])\n",
    "        if texto:\n",
    "            conll_pre.append(texto)\n",
    "            \n",
    "            \n",
    "    print(\"CoNLL - Num documentos: \", len(conll_pre))\n",
    "    return conll_pre\n",
    "    \n",
    "\n",
    "    \n",
    "def generate_corpus_from_conll(path='conll'):\n",
    "    conll_pre = get_conll_pre(path=path)\n",
    "    corpus = []\n",
    "    for text in conll_pre:\n",
    "        new_text = []\n",
    "        for token in text.split('\\n'):\n",
    "            new_text.append(token.split(' ')[0])\n",
    "        \n",
    "        corpus.append(' '.join(new_text))\n",
    "        \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d754b2a-032f-4f20-ac01-2f094aee6dc1",
   "metadata": {},
   "source": [
    "#### Wikiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68e2baea-5316-4634-8ad2-37e399a0e58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wikiner - Num documentos:  286953\n",
      "286953\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nnlp = spacy.load(\"es_core_news_lg\")\\ndocs = nlp.pipe(generate_corpus_from_wikiner(corpus_pre))\\n\\ndoc = next(docs)\\n\\nprint(doc.ents)\\ndel(nlp)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_wikiner_pre(path='wikiner'):\n",
    "    with open(f'{path}/aij-wikiner-es-wp2', 'r') as f:\n",
    "        wikiner2 = f.read()\n",
    "\n",
    "    with open(f'{path}/aij-wikiner-es-wp3', 'r') as f:\n",
    "        wikiner3 = f.read()\n",
    "\n",
    "    wikiner_pre = f'{wikiner2}\\n\\n{wikiner3}'.split('\\n')\n",
    "    print(\"Wikiner - Num documentos: \", len(wikiner_pre))\n",
    "    \n",
    "    return wikiner_pre\n",
    "\n",
    "\n",
    "def generate_corpus_from_wikiner(path='wikiner'):\n",
    "    wikiner_pre = get_wikiner_pre(path=path)\n",
    "    corpus = []\n",
    "    for i,text in enumerate(wikiner_pre):\n",
    "        new_text = []\n",
    "        text = text.replace('\\n',' ')\n",
    "        for j, token in enumerate(text.split(' ')):\n",
    "            new_text.append(token.split('|')[0])\n",
    "\n",
    "        corpus.append(' '.join(new_text))\n",
    "        \n",
    "    return corpus\n",
    "\n",
    "#print(\"Original: \",get_wikiner_pre()[0])\n",
    "print()\n",
    "#print(\"Processed: \",generate_corpus_from_wikiner()[0])\n",
    "\n",
    "corpus = generate_corpus_from_wikiner()\n",
    "\n",
    "print(len(corpus))\n",
    "\"\"\"\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_lg\")\n",
    "docs = nlp.pipe(generate_corpus_from_wikiner(corpus_pre))\n",
    "\n",
    "doc = next(docs)\n",
    "\n",
    "print(doc.ents)\n",
    "del(nlp)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c006202d-eddb-4f28-9cad-e51ac0fc24b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   \\nprint(\"Original: \",corpus_pre[0])\\nprint()\\nprint(\"Processed: \",generate_corpus_from_wikiner(corpus_pre)[0])\\n  \\n\\nnlp = spacy.load(\"es_core_news_lg\")\\ndocs = nlp.pipe(generate_corpus_from_wikiner(corpus_pre))\\n\\ndoc = next(docs)\\n\\nprint(doc.ents)\\ndel(nlp)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_spacy_binary_format_wikinier(corpus_pre):\n",
    "    \"\"\"\n",
    "        No funciona (05/08/2022)\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for text_id in tqdm(range(len(corpus_pre))):\n",
    "        if corpus_pre[text_id] == '':\n",
    "            continue \n",
    "            \n",
    "        paragraphs = [] \n",
    "        token_id = 0\n",
    "        offset = 0\n",
    "        start = 0\n",
    "        end = 0\n",
    "        for paragraph_id in range(len(corpus_pre[text_id].split('\\n'))):\n",
    "            raw_pre = corpus_pre[text_id].split('\\n')[paragraph_id]\n",
    "            \n",
    "            sentences = []\n",
    "            for sentence_id in range(len(raw_pre.split('.|FS|O'))):\n",
    "                tokens = []\n",
    "                curr_entity = []  # list of tokens (will become an entity)\n",
    "                last_ner = ''\n",
    "                for token_idx in range(len(raw_pre.split('.|FS|O')[sentence_id].split(' '))):\n",
    "                    token_labeled = raw_pre.split('.|FS|O')[sentence_id].split(' ')[token_idx]\n",
    "                    if len(token_labeled)<=1:\n",
    "                        continue\n",
    "                       \n",
    "                    \n",
    "                    try:\n",
    "                        curr_tag = token_labeled.split('|')[1]\n",
    "                    except:\n",
    "                        print(token_labeled)\n",
    "                        print(token_labeled.split('|'))\n",
    "                        print(raw_pre.split('.|FS|O')[sentence_id].split(' '))\n",
    "                        raise\n",
    "                        \n",
    "                    curr_ner = token_labeled.split('|')[2]\n",
    "                    curr_token = token_labeled.split('|')[0]\n",
    "                    dep = ''\n",
    "                    head = offset\n",
    "                    orth = '' \n",
    "                    \n",
    "                    if len(curr_ner)==0:\n",
    "                        print(token_labeled)\n",
    "\n",
    "                    if ('O' == curr_ner or last_ner.split('-')[-1]!=curr_ner.split('-')[-1]) and len(curr_entity)>0:\n",
    "                        # Cambio de entidad                   \n",
    "                        curr_entity = []\n",
    "                        last_ner = ''\n",
    "                    elif 'B' == curr_ner[0]:\n",
    "                        # Comienzo de entidad\n",
    "                        end = offset-1\n",
    "                        start = offset\n",
    "                        curr_entity = [curr_token] \n",
    "                        last_ner = curr_ner\n",
    "\n",
    "                    elif 'I' == curr_ner[0]:\n",
    "                        # Comienzo o continuación de entidad (depende del dataset)\n",
    "                        if len(curr_entity)>0:\n",
    "                            curr_ner = 'B-'+curr_ner.split('-')[-1]\n",
    "                            \n",
    "                        curr_entity.append(curr_token)\n",
    "                        last_ner = curr_ner\n",
    "                        if start == 0:\n",
    "                            start = offset\n",
    "\n",
    "                    offset += len(curr_token) +1\n",
    "\n",
    "                   \n",
    "                    token= {\n",
    "                            \"id\": token_id,\n",
    "                            \"dep\": dep,\n",
    "                            \"head\": head,\n",
    "                            \"tag\": curr_tag,\n",
    "                            \"orth\": orth,\n",
    "                            \"ner\":curr_ner\n",
    "                        }\n",
    "                    tokens.append(token)\n",
    "                        \n",
    "                        \n",
    "                    token_id +=1\n",
    "                \n",
    "                sentence = { \"tokens\": tokens}\n",
    "                sentences.append(sentence)\n",
    "                \n",
    "            aux = []\n",
    "            for token_aux in raw_pre.split(' '):\n",
    "                aux.append(token_aux.split('|')[0])\n",
    "            raw = ' '.join(aux)\n",
    "                \n",
    "            paragraph = { \"raw\": raw,\n",
    "                          \"sentences\": sentences}\n",
    "            paragraphs.append(paragraph)\n",
    "                           \n",
    "        data.append({\"id\":text_id, \"paragraphs\": paragraphs})\n",
    "        \n",
    "    \n",
    "                           \n",
    "    with open('wikiner.spacy','w') as f:\n",
    "        json.dump(data, f)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\"\"\"   \n",
    "print(\"Original: \",corpus_pre[0])\n",
    "print()\n",
    "print(\"Processed: \",generate_corpus_from_wikiner(corpus_pre)[0])\n",
    "  \n",
    "\n",
    "nlp = spacy.load(\"es_core_news_lg\")\n",
    "docs = nlp.pipe(generate_corpus_from_wikiner(corpus_pre))\n",
    "\n",
    "doc = next(docs)\n",
    "\n",
    "print(doc.ents)\n",
    "del(nlp)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e3abf13-1979-4438-a443-25aa26bce6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_spacy_binary_format_wikinier(corpus_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9439771-4640-4a4d-aec7-42089458436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_metrics(dataset, modelo, sufijo_nombre_archivo):\n",
    "    TPTNFPFN_file = f'{dataset}/TPTNFPFN_{modelo}{sufijo_nombre_archivo}.json'\n",
    "    \n",
    "    with open(TPTNFPFN_file) as f:\n",
    "        dic_confusion = json.load(f)\n",
    "        \n",
    "    # Detection\n",
    "    dic_det =  dic_confusion['detection']\n",
    "    #accuracy_det = (dic_det['TP']+dic_det['TN'])/(dic_det['TP']+dic_det['TN']+dic_det['FP']+dic_det['FN'])\n",
    "    recall_det = (dic_det['TP'])/(dic_det['TP']+dic_det['FN']) if dic_det['TP']+dic_det['FN']>0 else 0\n",
    "    precision_det = (dic_det['TP'])/(dic_det['TP']+dic_det['FP']) if dic_det['TP']+dic_det['FP']>0 else 0\n",
    "    f1score_det = 2*(recall_det*precision_det)/(recall_det+precision_det) if recall_det+precision_det>0 else 0\n",
    "    \n",
    "    jaccard_det = dic_det['TP']/(dic_det['TP']+dic_det['FP']+dic_det['FN']) if dic_det['TP']+dic_det['FP']+dic_det['FN']>0 else 0 # IoU \n",
    "    \n",
    "    dic_metrics_det = {\n",
    "                        #'acc': accuracy_det,\n",
    "                        'rec': recall_det,\n",
    "                        'prec': precision_det,\n",
    "                        'f1': f1score_det,\n",
    "                        'jac': jaccard_det\n",
    "                        }\n",
    "    \n",
    "    # Classification    \n",
    "    dic_clf = dic_confusion['classification']\n",
    "    dic_metrics_clf = {}\n",
    "    for class_ in dic_clf.keys():\n",
    "        dic_class = dic_clf[f'{class_}']\n",
    "        \n",
    "\n",
    "        #accuracy_class = (dic_class['TP']+dic_class['TN'])/(dic_class['TP']+dic_class['TN']+dic_class['FP']+dic_class['FN'])\n",
    "        recall_class = (dic_class['TP'])/(dic_class['TP']+dic_class['FN']) if dic_class['TP']+dic_class['FN']>0 else 0\n",
    "        precision_class = (dic_class['TP'])/(dic_class['TP']+dic_class['FP']) if dic_class['TP']+dic_class['FP']>0 else 0\n",
    "        f1score_class = 2*(recall_class*precision_class)/(recall_class+precision_class) if recall_class+precision_class>0 else 0\n",
    "\n",
    "        jaccard_class = dic_class['TP']/(dic_class['TP']+dic_class['FP']+dic_class['FN']) if dic_class['TP']+dic_class['FP']+dic_class['FN']>0 else 0 # IoU\n",
    "\n",
    "        \n",
    "        \n",
    "        dic_metrics_clf[f'{class_}'] = {\n",
    "                                        #'acc': accuracy_class,\n",
    "                                        'rec': recall_class,\n",
    "                                        'prec': precision_class,\n",
    "                                        'f1': f1score_class,\n",
    "                                        'jac': jaccard_class\n",
    "                                        }\n",
    "    # Save results\n",
    "    dic_results = {\n",
    "                   'detection': dic_metrics_det,\n",
    "                   'classification': dic_metrics_clf\n",
    "                  }\n",
    "    \n",
    "    \n",
    "    with open(f'{dataset}/metrics_{modelo}{sufijo_nombre_archivo}.json', 'w') as f:\n",
    "        json.dump(dic_results, f)\n",
    "    \n",
    "    \n",
    "    # Show metrics\n",
    "    print(\"\\n\")\n",
    "    print(('='*10+'\\n')*3)\n",
    "    print(f\"{dataset.upper()} - {modelo.upper()}\")\n",
    "    \n",
    "    print('DETECTION:')\n",
    "    for metric in dic_metrics_det.keys():\n",
    "        print('\\t', metric.upper(),': ',dic_metrics_det[f'{metric}'])\n",
    "        \n",
    "        \n",
    "    \n",
    "    print('CLASSIFICATION:')\n",
    "    for class_ in dic_metrics_clf.keys():\n",
    "        print('\\t Class ', class_.upper())\n",
    "        for metric in dic_metrics_clf[f'{class_}'].keys():\n",
    "            print('\\t\\t', metric.upper(),': ',dic_metrics_clf[f'{class_}'][f'{metric}'])    \n",
    "    \n",
    "    print(('='*10+'\\n')*3)\n",
    "    \n",
    "    return dic_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "152064e8-e47f-4771-91bb-ab29c14c0621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_true_named_entities(nombre_corpus: str):\n",
    "    path = f'{nombre_corpus}/true_named_entities.pkl'\n",
    "    if os.path.exists(path):\n",
    "        with open(path, 'rb') as f:\n",
    "            true_named_entities = pickle.load(f)\n",
    "            \n",
    "    else:\n",
    "        if nombre_corpus=='wikiner':\n",
    "            corpus_pre = get_wikiner_pre(path=nombre_corpus)\n",
    "            corpus = generate_corpus_from_wikiner()\n",
    "            split_token = ' '\n",
    "            split_ner = '|' \n",
    "        elif nombre_corpus=='conll':\n",
    "            corpus_pre = get_conll_pre(path=nombre_corpus)\n",
    "            corpus = generate_corpus_from_conll()\n",
    "            split_token = '\\n'\n",
    "            split_ner = ' '\n",
    "        else:\n",
    "            # No contemplado\n",
    "            raise\n",
    "            \n",
    "            \n",
    "            \n",
    "        true_named_entities = [] # axis 0: textos, axis 1: pares de entidades [entidad, tipo]\n",
    "        for i in tqdm(range(len(corpus_pre))):\n",
    "            text_processed = corpus[i]\n",
    "            text = corpus_pre[i].strip().replace('\\n', split_token)\n",
    "            \n",
    "            text_true_ne = [] # list of entities\n",
    "\n",
    "            curr_entity = []  # list of tokens (will become an entity)\n",
    "            last_tag = ''\n",
    "            offset = 0\n",
    "            start = 0\n",
    "            for token_labeled in text.split(split_token):\n",
    "                if offset>0:\n",
    "                    offset += 1\n",
    "                    \n",
    "                if len(token_labeled)==0:\n",
    "                    continue\n",
    "                    \n",
    "                curr_tag = token_labeled.split(split_ner)[-1].strip().upper()\n",
    "                curr_token = token_labeled.split(split_ner)[0]\n",
    "                \n",
    "                if ('O' == curr_tag[0] or last_tag.split('-')[-1]!=curr_tag.split('-')[-1]) and len(curr_entity)>0:\n",
    "                    # Cambio de entidad\n",
    "                    ent_name = ' '.join(curr_entity)                    \n",
    "                    text_true_ne.append([ent_name, last_tag.split('-')[-1], start, start+len(ent_name)])   \n",
    "                    start = 0\n",
    "                    end = 0\n",
    "                    \n",
    "                    curr_entity = []\n",
    "                    last_tag = ''\n",
    "\n",
    "                elif 'B' == curr_tag[0]:\n",
    "                    # Comienzo de entidad\n",
    "                    if len(curr_entity)>0:\n",
    "                        ent_name = ' '.join(curr_entity)\n",
    "                        text_true_ne.append([ent_name, last_tag.split('-')[-1], start, start+len(ent_name)])\n",
    "                        \n",
    "                    start = offset\n",
    "                    curr_entity = [curr_token] \n",
    "                    last_tag = curr_tag\n",
    "\n",
    "                elif 'I' == curr_tag[0]:\n",
    "                    # Comienzo o continuación de entidad (depende del dataset)\n",
    "                    curr_entity.append(curr_token)\n",
    "                    last_tag = curr_tag\n",
    "                    if start == 0:\n",
    "                        start = offset\n",
    "                    \n",
    "                \n",
    "                offset += len(curr_token)\n",
    "\n",
    "                \n",
    "\n",
    "            true_named_entities.append(text_true_ne)\n",
    "        \n",
    "        with open(path,'wb') as f:\n",
    "            pickle.dump(true_named_entities, f)\n",
    "        \n",
    "    return true_named_entities\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def predict_named_entities(modelo: str, nombre_corpus: str):\n",
    "    # Load model\n",
    "    path = f'{nombre_corpus}/{modelo}_pred_named_entities.pkl'\n",
    "    if os.path.exists(path):        \n",
    "        with open(path, 'rb') as f:\n",
    "            pred_named_entities = pickle.load(f)\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        if nombre_corpus=='wikiner':\n",
    "            corpus_pre = get_wikiner_pre(path=nombre_corpus)\n",
    "            generate_corpus = generate_corpus_from_wikiner\n",
    "        elif nombre_corpus=='conll':\n",
    "            corpus_pre = get_conll_pre(path=nombre_corpus)\n",
    "            generate_corpus = generate_corpus_from_conll\n",
    "        else:\n",
    "            # No contemplado\n",
    "            raise\n",
    "        \n",
    "        corpus = generate_corpus()\n",
    "        \n",
    "        if modelo == 'spacy':\n",
    "            nlp = spacy.load(\"es_core_news_lg\")\n",
    "            docs = nlp.pipe(generate_corpus())\n",
    "\n",
    "        elif modelo == 'flair':\n",
    "            tagger = SequenceTagger.load('es-ner-large')\n",
    "            \n",
    "        elif modelo == 'gc': # google.cloud.language_v1\n",
    "            client = language_v1.LanguageServiceClient()\n",
    "\n",
    "            # Available types: PLAIN_TEXT, HTML\n",
    "            type_ = language_v1.Document.Type.PLAIN_TEXT\n",
    "\n",
    "            # Optional. If not specified, the language is automatically detected.\n",
    "            # For list of supported languages:\n",
    "            # https://cloud.google.com/natural-language/docs/languages\n",
    "            language = \"es\"\n",
    "\n",
    "            # Available values: NONE, UTF8, UTF16, UTF32\n",
    "            encoding_type = language_v1.EncodingType.UTF32\n",
    "\n",
    "        else:\n",
    "            # No contemplado\n",
    "            raise\n",
    "\n",
    "        pred_named_entities = []\n",
    "        for i, text in enumerate(tqdm(corpus_pre[:])):                \n",
    "            if modelo == 'spacy':\n",
    "                doc = next(docs)\n",
    "                text_pred_ne = [ [str(ent), ent.label_, ent.start_char, ent.end_char] for ent in doc.ents]\n",
    "\n",
    "            elif modelo == 'flair':\n",
    "                s = corpus[i]\n",
    "                sentence = Sentence(s)\n",
    "\n",
    "                # predict NER tags\n",
    "                tagger.predict(sentence)\n",
    "                entidades = sentence.get_spans('ner')\n",
    "                text_pred_ne = [[str(ent.text) , ent.get_label(\"ner\").value, ent.start_position, ent.end_position] for ent in sentence.get_spans('ner')]\n",
    "            \n",
    "            elif modelo == 'gc':\n",
    "                document = {\"content\": corpus[i], \"type_\": type_, \"language\": language}\n",
    "                response = client.analyze_entities(request = {'document': document, 'encoding_type': encoding_type})  \n",
    "                \n",
    "                text_pred_ne = []\n",
    "                for entity in response.entities:\n",
    "                    tipo = language_v1.Entity.Type(entity.type_).name\n",
    "                    if tipo == 'UNKNOWN':\n",
    "                        continue\n",
    "                    elif tipo == 'PERSON':\n",
    "                        tipo = 'PER'\n",
    "                    elif tipo == 'LOCATION':\n",
    "                        tipo = 'LOC'\n",
    "                    elif tipo == 'ORGANIZATION':\n",
    "                        tipo = 'ORG'\n",
    "                    else:\n",
    "                        tipo = 'MISC'\n",
    "                    \n",
    "                    for mention in entity.mentions:                            \n",
    "                        text_pred_ne.append( [str(mention.text.content), tipo, mention.text.begin_offset, mention.text.begin_offset+len(str(mention.text.content))] )\n",
    "                        \n",
    "                    text_pred_ne.sort(key = lambda i: i[2])\n",
    "\n",
    "            pred_named_entities.append(text_pred_ne)\n",
    "            \n",
    "        with open(path,'wb') as f:\n",
    "            pickle.dump(pred_named_entities, f)\n",
    "\n",
    "    return pred_named_entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "d08616d1-bdd2-4a7c-b18f-5ea2fd3c87fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoNLL - Num documentos:  404\n",
      "CoNLL - Num documentos:  404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 404/404 [01:52<00:00,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404\n",
      "404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "a = extract_true_named_entities('conll')\n",
    "b = predict_named_entities('gc', 'conll')\n",
    "\n",
    "print(len(a))\n",
    "print(len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "472eaf9a-d92b-42ec-b9f6-262e63ae1586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6645\n",
      "{'ORG', 'MISC', 'PER', 'LOC'}\n"
     ]
    }
   ],
   "source": [
    "a = extract_true_named_entities('conll')\n",
    "tipos = []\n",
    "for t in a:\n",
    "    for token in t:\n",
    "        tipos.append(token[1])\n",
    "        \n",
    "print(len(tipos))\n",
    "print(set(tipos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "fa9eb752-d82f-41bf-a164-5ff4eff9da2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_v0(corpus_pre, modelo: str, nombre_corpus: str):\n",
    "    \"\"\"\n",
    "        Primera versión de test.\n",
    "        \n",
    "        En esta versión se procesa el texto a la vez que se \n",
    "        va testeando. Se tienen en cuenta, por lo tanto, los\n",
    "        True Negative. El testeo se hace a nivel de TOKEN.\n",
    "        \n",
    "        Problemas:\n",
    "            - Mucho tiempo para testear nuevas métricas\n",
    "    \"\"\"\n",
    "    modelo = modelo.lower().strip()\n",
    "    nombre_corpus = nombre_corpus.lower().strip()\n",
    "    assert(modelo == 'spacy' or modelo == 'flair')\n",
    "    \n",
    "    \n",
    "        \n",
    "    # Diccionario base\n",
    "    dic_TPTNFPFN = {'TP':0,'TN':0,'FP':0,'FN':0}\n",
    "\n",
    "    # Detección de entidades\n",
    "    dic_detection = copy.deepcopy(dic_TPTNFPFN)\n",
    "    \n",
    "    # Clasificación de entidades: cada elemento del diccionario será un diccionario con claves TP, TN, FP, FN\n",
    "    dic_clases = {\n",
    "                    'PER': copy.deepcopy(dic_TPTNFPFN),\n",
    "                    'MISC': copy.deepcopy(dic_TPTNFPFN),\n",
    "                    'LOC': copy.deepcopy(dic_TPTNFPFN),\n",
    "                    'ORG': copy.deepcopy(dic_TPTNFPFN)\n",
    "                }\n",
    "            \n",
    "            \n",
    "    if modelo == 'spacy':\n",
    "        nlp = spacy.load(\"es_core_news_lg\")\n",
    "        docs = nlp.pipe(generate_corpus_from_wikiner(corpus_pre))\n",
    "\n",
    "    elif modelo == 'flair':\n",
    "        tagger = SequenceTagger.load('es-ner-large')\n",
    "\n",
    "    pred_named_entities = []\n",
    "    for i, text in enumerate(tqdm(corpus_pre)):           \n",
    "        \n",
    "        if modelo == 'spacy':\n",
    "            doc = next(docs)\n",
    "            entidades = doc.ents\n",
    "            \n",
    "        elif modelo == 'flair':\n",
    "            s = generate_corpus_from_wikiner([text])[0]\n",
    "            sentence = Sentence(s)\n",
    "            \n",
    "            # predict NER tags\n",
    "            tagger.predict(sentence)\n",
    "            entidades = sentence.get_spans('ner')\n",
    "        \n",
    "\n",
    "        curr_token = 0\n",
    "        tokens = text.split(' ')\n",
    "        last_token = -1 # Va a permitir no sumar TN_det y FN_det cuando estudiemos una misma entidad\n",
    "                        # ya que un problema que había era que un \"token\" original podría ser: \"UGR/Universidad|NC|ORG\"\n",
    "                        # y el modelo los predice como dos entidades distintas, entonces no podemos saltar de token en \"UGR\"\n",
    "                        # con curr_token+=1, porque en la siguiente interacción perderíamos la entidad \"Universidad\".\n",
    "                        # Pero tampoco tiene sentido contarlo como error si es un token normal que no da problemas. Por eso,\n",
    "                        # con last_token conseguimos no contar esos errores, pero sí inspeccionar de nuevo el último token\n",
    "                        # para contarlo como entidad positiva en caso de serlo.\n",
    "        for ent_ in entidades:\n",
    "            # Get the label entity prediction\n",
    "            if modelo == 'spacy':\n",
    "                pred_label = ent_.label_\n",
    "            elif modelo == 'flair':\n",
    "                pred_label = ent_.get_label(\"ner\").value\n",
    "                ent_ = ent_.text\n",
    "            \n",
    "            \n",
    "            for ent in str(ent_).split(' '):\n",
    "                encontrado = str(ent) in tokens[curr_token].split('|')[0] # {!=} --> {not in}\n",
    "                \n",
    "                while not encontrado:   \n",
    "                    \n",
    "                    if curr_token > last_token:\n",
    "                        if 'O' == tokens[curr_token].split('|')[-1].strip().upper():\n",
    "                            dic_detection['TN']+= 0\n",
    "                        else:\n",
    "                            dic_detection['FN'] += 1\n",
    "                        \n",
    "                    curr_token+=1\n",
    "                    try:\n",
    "                        encontrado = str(ent) in tokens[curr_token].split('|')[0]\n",
    "                    except:\n",
    "                        # debug\n",
    "                        print('ent_.text', str(ent_))\n",
    "                        print('ent',str(ent))\n",
    "                        print('curr_token',curr_token)\n",
    "                        print('tokens[curr_token-1]',tokens[curr_token-1])\n",
    "                        print('tokens', tokens)\n",
    "                        raise # throw an error\n",
    "                    \n",
    "                last_token = curr_token \n",
    "                actual_label = tokens[curr_token].split('|')[-1].strip().upper().split('-')[-1]\n",
    "                        \n",
    "                if 'O' == actual_label:\n",
    "                    dic_detection['FP'] += 1 # una NO entidad ha sido detectada como entidad\n",
    "                else:\n",
    "                    dic_detection['TP'] += 1 # una entidad ha sido detectada como entidad\n",
    "                    \n",
    "                    if pred_label in actual_label:\n",
    "                        try:\n",
    "                            # una entidad ha sido detectada como entidad y además se ha clasificado bien\n",
    "                            # Sumamos los TP de esa clase de entidad\n",
    "                            dic_clases[f'{pred_label}']['TP'] += 1 \n",
    "                        except:\n",
    "                            dic_clases[f'{pred_label}']['TP'] = 1 \n",
    "                            \n",
    "                        for key in dic_clases.keys():\n",
    "                            # Sumamos como TN al resto de clases de entidad\n",
    "                            try:\n",
    "                                # una entidad ha sido detectada como entidad y además se ha clasificado bien\n",
    "                                dic_clases[f'{key}']['TN'] += 1 \n",
    "                            except:\n",
    "                                dic_clases[f'{key}']['TN'] = 1 \n",
    "                    else:\n",
    "                        # una entidad ha sido detectada como entidad y PERO NO se ha clasificado bien\n",
    "                        # Sumamos los FP de la clase predicha\n",
    "                        try:\n",
    "                            dic_clases[f'{pred_label}']['FP'] += 1 \n",
    "                        except:\n",
    "                            dic_clases[f'{pred_label}']['FP'] = 1 \n",
    "                            \n",
    "                            \n",
    "                        # Sumamos los FN de la clase real\n",
    "                        try:\n",
    "                            dic_clases[f'{actual_label}']['FN'] += 1 \n",
    "                        except:\n",
    "                            dic_clases[f'{actual_label}']['FN'] = 1 \n",
    "\n",
    "        \n",
    "    \n",
    "    dic_confusion = { 'detection': dic_detection,\n",
    "                     'classification': dic_clases\n",
    "                    }\n",
    "    \n",
    "    \n",
    "    with open(f'{nombre_corpus}/TPTNFPFN_{modelo}.json', 'w') as f:\n",
    "        json.dump(dic_confusion, f)\n",
    "        \n",
    "    dic_results = show_metrics(nombre_corpus, modelo)\n",
    "        \n",
    "    return dic_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "86ff8133-fcf9-414b-9df6-450b869a100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_v1(true_named_entities, pred_named_entities, modelo: str, nombre_corpus: str, clases_ignoradas=[], sufijo_nombre_archivo = \"\"):\n",
    "    \"\"\"\n",
    "        Segunda versión de test.\n",
    "        \n",
    "        En esta versión se mejora el tiempo de testeo \n",
    "        guardando en disco la extracción de entidades \n",
    "        echas por el modelo y la extracción de las \n",
    "        entidades reales del texto, haciendo que solo\n",
    "        sea necesario calcularse en su primera ejecución.\n",
    "        \n",
    "        Además, se han eliminado las STOPWORDS de las entidades.\n",
    "    \"\"\"\n",
    "    \n",
    "    modelo = modelo.lower().strip()\n",
    "    nombre_corpus = nombre_corpus.lower().strip()\n",
    "    assert(modelo == 'spacy' or modelo == 'flair' or modelo == 'gc' or modelo == 'test')\n",
    "    \n",
    "    \n",
    "    dic_errores = {'detection': [],\n",
    "                   'classification':{\n",
    "                                        'LOC': [],\n",
    "                                        'MISC': [],\n",
    "                                        'ORG': [],\n",
    "                                        'PER': []\n",
    "                                    }\n",
    "                  }\n",
    "    \n",
    "    # Detección de entidades\n",
    "    dic_detection = {'TP':0,'TN':0,'FP':0,'FN':0}\n",
    "    \n",
    "    # Clasificación de entidades: cada elemento del diccionario será un diccionario con claves TP, TN, FP, FN\n",
    "    dic_clases = {\n",
    "                    'LOC': {'TP':0,'TN':0,'FP':0,'FN':0},\n",
    "                    'MISC': {'TP':0,'TN':0,'FP':0,'FN':0},\n",
    "                    'ORG': {'TP':0,'TN':0,'FP':0,'FN':0},\n",
    "                    'PER': {'TP':0,'TN':0,'FP':0,'FN':0}\n",
    "                }\n",
    "    \n",
    "    for texto_idx in tqdm(range(len(pred_named_entities))):\n",
    "        last_true_ent_founded_idx = 0\n",
    "        falsos_negativos_acumulados = 0 # declaración \n",
    "        \n",
    "        if len(pred_named_entities[texto_idx])==0 and len(true_named_entities[texto_idx])==0:\n",
    "            continue\n",
    "        \n",
    "        overlapped_pred_entities = [-1 for i in range(len(pred_named_entities[texto_idx]))]\n",
    "        overlapped_true_entities = [-1 for i in range(len(true_named_entities[texto_idx]))]\n",
    "        \n",
    "        #print(overlapped_pred_entities, '\\n', overlapped_true_entities)\n",
    "        pred_ent_labeled_idx = 0\n",
    "        true_ent_labeled_idx = 0\n",
    "        avanza_pred_avanza_true = 2 # 0 avanza pred, 1 avanza true, 2 avanzan las dos\n",
    "        fin_pred = False\n",
    "        fin_true = False \n",
    "            \n",
    "        while not fin_pred or not fin_true:\n",
    "            # Predicted\n",
    "            if len(pred_named_entities[texto_idx])>0:\n",
    "                pred_ent_labeled = pred_named_entities[texto_idx][pred_ent_labeled_idx]\n",
    "            else:\n",
    "                fin_pred = True\n",
    "                pred_ent_labeled = ['', '_NAN_', 1e5, 1+1e5] # entrada auxiliar\n",
    "            #pred_ent = remove_stopwords(remove_punctuation(pred_ent_labeled[0].lower(), sub=''))\n",
    "            pred_ent = remove_stopwords(pred_ent_labeled[0].lower())\n",
    "            pred_label = pred_ent_labeled[1]\n",
    "            pred_start = pred_ent_labeled[2]\n",
    "            pred_end = pred_ent_labeled[3]\n",
    "            \n",
    "            saltamos_pred = len(pred_ent)==0 or pred_label in clases_ignoradas\n",
    "            \n",
    "            if avanza_pred_avanza_true==0 or avanza_pred_avanza_true==2:\n",
    "                pred_overlapped = [False for token in pred_ent.split(' ')]\n",
    "            \n",
    "            # Actual\n",
    "            if len(true_named_entities[texto_idx])>0:\n",
    "                true_ent_labeled = true_named_entities[texto_idx][true_ent_labeled_idx]\n",
    "            else:\n",
    "                fin_true = True\n",
    "                true_ent_labeled = ['', 'O', 1e5, 1+1e5] # entrada auxiliar\n",
    "            \n",
    "            #true_ent = remove_stopwords(remove_punctuation(true_ent_labeled[0].lower(), sub=''))\n",
    "            true_ent = remove_stopwords(true_ent_labeled[0].lower())\n",
    "            true_label = true_ent_labeled[1]\n",
    "            true_start = true_ent_labeled[2]\n",
    "            true_end = true_ent_labeled[3]\n",
    "            \n",
    "            saltamos_true = len(true_ent)==0 or true_label in clases_ignoradas\n",
    "\n",
    "                \n",
    "            if avanza_pred_avanza_true==1 or avanza_pred_avanza_true==2:\n",
    "                true_overlapped = [False for token in true_ent.split(' ')]\n",
    "               \n",
    "            \"\"\"\n",
    "            print(pred_ent, pred_end, '||', len(pred_named_entities[texto_idx]), '-', pred_ent_labeled_idx, fin_pred)\n",
    "            print(true_ent, true_end, '||', len(true_named_entities[texto_idx]), '-', true_ent_labeled_idx, fin_true)\n",
    "            print()\n",
    "            \"\"\"\n",
    "                \n",
    "            # Vemos si hay solapamiento\n",
    "            if ((pred_start <= true_start and pred_end > true_start) or (true_start <=pred_start and true_end > pred_start)) and (not saltamos_pred and not saltamos_true):\n",
    "                # Hay solapamiento\n",
    "                    \n",
    "                for pred_token_idx in range(len(pred_ent.split(' '))):\n",
    "                    pred_token = pred_ent.split(' ')[pred_token_idx]\n",
    "                    \n",
    "                    # Comprobamos si el token está en ambas entidades y si no ha sido ya marcado como overlapped\n",
    "                    if len(pred_token)>0 and pred_token in true_ent and not pred_overlapped[pred_token_idx]:\n",
    "                        pred_overlapped[pred_token_idx] = True\n",
    "                                            \n",
    "                        # CLASIFICACIÓN ( solo lo hacemos en este bucle ya que con el siguiente se duplicarían las acciones )\n",
    "                        if pred_label in true_label: # por ej. ORG in I-ORG => True. \n",
    "                            # Sumamos TP al pred_label y sumamos TN al resto\n",
    "                            dic_clases[f'{pred_label}']['TP'] += 1\n",
    "                            \n",
    "                            \n",
    "                            for key in dic_clases.keys():\n",
    "                                if key != pred_label:\n",
    "                                    dic_clases[f'{key}']['TN'] += 1\n",
    "                        else:\n",
    "                            # Sumamos FP al pred_label y FN al true_label\n",
    "                            dic_clases[f'{pred_label}']['FP'] += 1\n",
    "                            dic_clases[f'{true_label.split(\"-\")[-1]}']['FN'] += 1\n",
    "                            \n",
    "                            error = {\n",
    "                                        'True':{'ent': true_ent,'label':true_label}, \n",
    "                                        'Predicted':{'ent': pred_ent, 'token': pred_token, 'label':pred_label}\n",
    "                                    }\n",
    "                            dic_errores['classification'][f'{true_label.split(\"-\")[-1]}'].append(error)\n",
    "                            \n",
    "                            for key in dic_clases.keys():\n",
    "                                if key != pred_label and key != true_label.split(\"-\")[-1]:\n",
    "                                    dic_clases[f'{key}']['TN'] += 1\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                        \n",
    "                for true_token_idx in range(len(true_ent.split(' '))):\n",
    "                    true_token = true_ent.split(' ')[true_token_idx]\n",
    "                    \n",
    "                    # Comprobamos si el token está en ambas entidades y si no ha sido ya marcado como overlapped\n",
    "                    if len(true_token)>0 and true_token in pred_ent and not true_overlapped[true_token_idx]:\n",
    "                        true_overlapped[true_token_idx] = True\n",
    "                \n",
    "\n",
    "            \n",
    "                    \n",
    "            fin_pred = pred_ent_labeled_idx+1 >= len(pred_named_entities[texto_idx])\n",
    "            fin_true = true_ent_labeled_idx+1 >= len(true_named_entities[texto_idx])\n",
    "            \n",
    "            \n",
    "            # Avance de entidad\n",
    "            if (pred_end < true_end or fin_true or saltamos_pred) and not fin_pred:\n",
    "                # La real acaba después, hay que avanzar con la predicha\n",
    "                avanza_pred_avanza_true = 0\n",
    "\n",
    "            elif (pred_end > true_end or fin_pred or saltamos_true) and not fin_true:\n",
    "                # La predicha acaba después, hay que avanzar con la real\n",
    "                avanza_pred_avanza_true = 1\n",
    "\n",
    "            else:\n",
    "                # Acaban a la vez, avanzamos con las dos\n",
    "                avanza_pred_avanza_true = 2\n",
    "           \n",
    "            \n",
    "            \n",
    "            # Contabilización en el avance\n",
    "            error_added = False\n",
    "            if (fin_true and fin_pred) or avanza_pred_avanza_true==0 or avanza_pred_avanza_true==2:\n",
    "                # Avanza pred\n",
    "                if not saltamos_pred and not saltamos_true:\n",
    "                    # Sumamos los TP y FP de la entidad predicha\n",
    "                    suma = int(np.array(pred_overlapped).sum())\n",
    "                    dic_detection['TP'] += suma\n",
    "                    dic_detection['FP'] += (len(pred_overlapped)-suma)\n",
    "\n",
    "                    # Añadimos los errores\n",
    "                    if len(pred_overlapped)-suma>0: # Hay errores\n",
    "                        error = {\n",
    "                                 'True': {'ent': true_ent, 'label': true_label, 'start': true_start, 'end': true_end},\n",
    "                                 'Predicted': {'ent': pred_ent, 'label': pred_label, 'start': pred_start, 'end': pred_end}\n",
    "                                }\n",
    "                        dic_errores['detection'].append(error)\n",
    "                        error_added = True\n",
    "                    \n",
    "                if not fin_pred:\n",
    "                    pred_ent_labeled_idx += 1\n",
    "\n",
    "\n",
    "            if (fin_pred and fin_true) or avanza_pred_avanza_true==1 or avanza_pred_avanza_true==2:\n",
    "                # Avanza true\n",
    "                if not saltamos_pred and not saltamos_true:\n",
    "                    # Sumamos los TP y FP de la entidad predicha\n",
    "                    suma = int(np.array(true_overlapped).sum())\n",
    "                    dic_detection['TP'] += suma # comentado porque se suman dos veces\n",
    "                    dic_detection['FN'] += (len(true_overlapped)-suma)\n",
    "\n",
    "\n",
    "                    # Añadimos los errores\n",
    "                    if len(pred_overlapped)-suma>0 and not error_added: # Hay errores\n",
    "                        error = {\n",
    "                                 'True': {'ent': true_ent, 'label': true_label, 'start': true_start, 'end': true_end},\n",
    "                                 'Predicted': {'ent': pred_ent, 'label': pred_label, 'start': pred_start, 'end': pred_end}\n",
    "                                }\n",
    "                        dic_errores['detection'].append(error)\n",
    "                    \n",
    "                if not fin_true:\n",
    "                    true_ent_labeled_idx += 1                    \n",
    "\n",
    "                \n",
    "            \n",
    "    dic_detection['TP'] /=2  # Están duplicadas\n",
    "            \n",
    "    dic_confusion = { 'detection': dic_detection,\n",
    "                      'classification': dic_clases\n",
    "                    }\n",
    "        \n",
    "    os.makedirs(nombre_corpus, exist_ok=True)\n",
    "    with open(f'{nombre_corpus}/TPTNFPFN_{modelo}{sufijo_nombre_archivo}.json', 'w') as f:\n",
    "        json.dump(dic_confusion, f)\n",
    "        \n",
    "    with open(f'{nombre_corpus}/errores_{modelo}{sufijo_nombre_archivo}.json', 'w') as f:\n",
    "        json.dump(dic_errores, f)\n",
    "        \n",
    "        \n",
    "    dic_results = show_metrics(nombre_corpus, modelo, sufijo_nombre_archivo = sufijo_nombre_archivo)\n",
    "        \n",
    "    return dic_results\n",
    "\n",
    "\n",
    "def test_testv1():\n",
    "    \"\"\"\n",
    "        Función para comprobar el buen funcionamiento de \n",
    "        la función test_v1\n",
    "    \"\"\"\n",
    "    true_named_entities = [[[\"Madrid\", \"LOC\", 10,16], [\"El Quijote de la Mancha\", \"PER\", 20, 44]],\n",
    "                           [[\"Villanueva de la Serena\", \"LOC\", 11, 34]],\n",
    "                           [[\"Banco Santander\", \"MISC\", 0, 15]]]\n",
    "                           \n",
    "    pred_named_entities =[[[\"Madrid\", \"LOC\", 10,16], [\"El Quijote\", \"PER\", 20, 30],[\"Mancha\", \"LOC\", 38, 44]],\n",
    "                           [[\"Falso Posi\", \"MISC\", 0, 10], [\"Villanueva\", \"LOC\", 11, 21]],\n",
    "                           [[\"Santander\", \"MISC\", 6, 15]]]\n",
    "    \n",
    "    \n",
    "    test_v1(true_named_entities = true_named_entities,\n",
    "            pred_named_entities = pred_named_entities,\n",
    "            modelo = 'spacy', \n",
    "            nombre_corpus = 'test',\n",
    "            sufijo_nombre_archivo = '')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "d4b5d9f3-184b-411e-93a3-b4721f89abbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoNLL - Num documentos:  404\n",
      "Novillos de \" El Torreón \" , bien presentados y de buen juego . Los seis , nobles , con fijeza y movilidad , aplaudidos en el arrastre . Víctor de la Serna : pinchazo , estocada y tres descabellos ( silencio tras un aviso ) ; y pinchazo y estocada que escupe ( silencio ) . Sebastián Castella : estocada desprendida ( silencio tras un aviso ) ; y dos pinchazos , estocada y descabello ( silencio ) . Javier Castaño : estocada chalequera ( vuelta tras petición de oreja ) ; y estocada ( dos orejas ) . Cuadrillas : Domingo Siro saludó tras banderillear al tercero .\n"
     ]
    }
   ],
   "source": [
    "corpus = generate_corpus_from_conll()\n",
    "print(corpus[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "b0ef8d31-c9e7-4c70-918b-ba2dbf10bdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hola', 'que', 'haces', '', '?', '']\n"
     ]
    }
   ],
   "source": [
    "s = \"Hola que  haces   ? \"\n",
    "s2 = s.replace('  ',' ')\n",
    "print(s2.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "6f65d7ff-6216-49ac-80e5-bcfd119de7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 721.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "\n",
      "TEST - SPACY\n",
      "DETECTION:\n",
      "\t REC :  0.7142857142857143\n",
      "\t PREC :  0.7142857142857143\n",
      "\t F1 :  0.7142857142857143\n",
      "\t JAC :  0.5555555555555556\n",
      "CLASSIFICATION:\n",
      "\t Class  LOC\n",
      "\t\t REC :  1.0\n",
      "\t\t PREC :  0.6666666666666666\n",
      "\t\t F1 :  0.8\n",
      "\t\t JAC :  0.6666666666666666\n",
      "\t Class  MISC\n",
      "\t\t REC :  1.0\n",
      "\t\t PREC :  1.0\n",
      "\t\t F1 :  1.0\n",
      "\t\t JAC :  1.0\n",
      "\t Class  ORG\n",
      "\t\t REC :  0\n",
      "\t\t PREC :  0\n",
      "\t\t F1 :  0\n",
      "\t\t JAC :  0\n",
      "\t Class  PER\n",
      "\t\t REC :  0.5\n",
      "\t\t PREC :  1.0\n",
      "\t\t F1 :  0.6666666666666666\n",
      "\t\t JAC :  0.5\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_testv1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "5ac756ff-7ab4-4f64-b50e-ee5b6700c6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOC\n",
      "Hola\n",
      "['LOC', 'Hola']\n",
      "['multinacional', 'ORG', 3, 16]\n",
      "['española', 'LOC', 17, 25]\n",
      "['Telefónica', 'ORG', 26, 36]\n",
      "['un', 'MISC', 49, 51]\n",
      "['récord', 'MISC', 52, 58]\n",
      "['servicio', 'MISC', 79, 87]\n",
      "['tres millones', 'MISC', 88, 101]\n",
      "['líneas', 'MISC', 112, 118]\n",
      "['estado', 'LOC', 125, 131]\n",
      "['brasileño', 'LOC', 132, 141]\n",
      "['Sao Paulo', 'LOC', 145, 154]\n",
      "['control', 'MISC', 175, 182]\n",
      "['operadora', 'PER', 189, 198]\n",
      "['Telesp', 'ORG', 199, 205]\n",
      "['20', 'MISC', 211, 213]\n",
      "['presidente', 'PER', 237, 247]\n",
      "['Telefónica', 'ORG', 251, 261]\n",
      "['Brasil', 'LOC', 265, 271]\n",
      "['Fernando Xavier Ferreira', 'PER', 274, 298]\n",
      "['un', 'MISC', 304, 306]\n",
      "['balance', 'MISC', 307, 314]\n",
      "['gestión', 'MISC', 330, 337]\n",
      "['Telefónica', 'ORG', 341, 351]\n",
      "['Sao Paulo', 'LOC', 355, 364]\n",
      "['1998', 'MISC', 381, 385]\n",
      "['1998', 'MISC', 381, 385]\n",
      "['Ferreira', 'PER', 388, 396]\n",
      "['empresa', 'ORG', 412, 419]\n",
      "['metas', 'MISC', 442, 447]\n",
      "['órgano', 'ORG', 463, 469]\n",
      "['sector', 'MISC', 484, 490]\n",
      "['Agencia Nacional de Telecomunicaciones', 'ORG', 496, 534]\n",
      "['ANATEL', 'ORG', 537, 543]\n",
      "['universalización', 'MISC', 560, 576]\n",
      "['calidad', 'MISC', 579, 586]\n",
      "['servicio', 'MISC', 591, 599]\n",
      "['Telefónica', 'ORG', 613, 623]\n",
      "['un', 'MISC', 631, 633]\n",
      "['compromiso', 'MISC', 634, 644]\n",
      "['Brasil', 'LOC', 649, 655]\n",
      "['especial', 'MISC', 663, 671]\n",
      "['Sao Paulo', 'LOC', 676, 685]\n",
      "['1998', 'MISC', 689, 693]\n",
      "['1998', 'MISC', 689, 693]\n",
      "['privatización', 'MISC', 703, 716]\n",
      "['sistema', 'MISC', 721, 728]\n",
      "['Telebras', 'ORG', 729, 737]\n",
      "['cuentas', 'MISC', 768, 775]\n",
      "['Ferreira', 'PER', 785, 793]\n",
      "['acto', 'MISC', 800, 804]\n",
      "['instalación', 'MISC', 808, 819]\n",
      "['línea', 'MISC', 826, 831]\n",
      "['número', 'MISC', 832, 838]\n",
      "['tres millones', 'MISC', 839, 852]\n",
      "['gestión', 'MISC', 859, 866]\n",
      "['Telefónica', 'ORG', 870, 880]\n",
      "['acto', 'MISC', 910, 914]\n",
      "['consejero delegado', 'PER', 918, 936]\n",
      "['Telefónica Internacional', 'ORG', 940, 964]\n",
      "['Antonio Viana Baptista', 'PER', 967, 989]\n",
      "['ministro de Comunicaciones', 'PER', 995, 1021]\n",
      "['Brasil', 'LOC', 1025, 1031]\n",
      "['Joao Pimienta da Veiga', 'PER', 1034, 1056]\n",
      "['presidente', 'PER', 1062, 1072]\n",
      "['ANATEL', 'ORG', 1079, 1085]\n",
      "['Renato Navarro Guerreiro', 'PER', 1088, 1112]\n",
      "['secretario de Ciencia', 'PER', 1120, 1141]\n",
      "['Tecnología', 'MISC', 1143, 1153]\n",
      "['Estado', 'LOC', 1158, 1164]\n",
      "['Sao Paulo', 'LOC', 1168, 1177]\n",
      "['José Aníbal Peres de Pontes', 'PER', 1180, 1207]\n",
      "['tres millones', 'MISC', 1218, 1231]\n",
      "['líneas', 'MISC', 1242, 1248]\n",
      "['servicio', 'MISC', 1258, 1266]\n",
      "['diez millones', 'MISC', 1278, 1291]\n",
      "['total', 'MISC', 1295, 1300]\n",
      "['terminales', 'MISC', 1304, 1314]\n",
      "['Telefónica', 'ORG', 1325, 1335]\n",
      "['estado', 'LOC', 1342, 1348]\n",
      "['Sao Paulo', 'LOC', 1352, 1361]\n",
      "['región', 'LOC', 1388, 1394]\n",
      "['brasileña', 'LOC', 1395, 1404]\n",
      "['área', 'LOC', 1411, 1415]\n",
      "['densidad', 'MISC', 1426, 1434]\n",
      "['habitante', 'PER', 1450, 1459]\n",
      "['América Latina', 'LOC', 1463, 1477]\n",
      "['25,6', 'MISC', 1484, 1488]\n",
      "['líneas', 'MISC', 1489, 1495]\n",
      "['funcionamiento', 'MISC', 1499, 1513]\n",
      "['cien', 'MISC', 1523, 1527]\n",
      "['personas', 'PER', 1528, 1536]\n",
      "['datos', 'MISC', 1549, 1554]\n",
      "['Telefónica', 'ORG', 1574, 1584]\n",
      "['empresa', 'ORG', 1590, 1597]\n",
      "['Sao Paulo', 'LOC', 1613, 1622]\n",
      "['una', 'MISC', 1623, 1626]\n",
      "['marca', 'MISC', 1627, 1632]\n",
      "['expansión', 'MISC', 1647, 1656]\n",
      "['redes', 'MISC', 1660, 1665]\n",
      "['telefonía', 'MISC', 1669, 1678]\n",
      "['una', 'MISC', 1701, 1704]\n",
      "['línea', 'MISC', 1705, 1710]\n",
      "['15', 'MISC', 1716, 1718]\n",
      "['una', 'MISC', 1740, 1743]\n",
      "['media', 'MISC', 1744, 1749]\n",
      "['180.000', 'MISC', 1753, 1760]\n",
      "['180.000', 'MISC', 1753, 1760]\n",
      "['terminales', 'MISC', 1768, 1778]\n",
      "['empresa', 'ORG', 1796, 1803]\n",
      "['líneas', 'MISC', 1828, 1834]\n",
      "['un', 'MISC', 1837, 1839]\n",
      "['ritmo', 'MISC', 1840, 1845]\n",
      "['mundo', 'LOC', 1863, 1868]\n",
      "['Telefónica', 'ORG', 1872, 1882]\n",
      "['Sao Paulo', 'LOC', 1888, 1897]\n",
      "['2001', 'MISC', 1933, 1937]\n",
      "['2001', 'MISC', 1933, 1937]\n",
      "['metas', 'MISC', 1954, 1959]\n",
      "['2003', 'MISC', 1978, 1982]\n",
      "['2003', 'MISC', 1978, 1982]\n",
      "['EFE Viana Baptista', 'ORG', 1994, 2012]\n",
      "['objetivo', 'MISC', 2018, 2026]\n",
      "['empresa', 'ORG', 2033, 2040]\n",
      "['2001', 'MISC', 2073, 2077]\n",
      "['2001', 'MISC', 2073, 2077]\n",
      "['tres millones', 'MISC', 2084, 2097]\n",
      "['líneas', 'MISC', 2108, 2114]\n",
      "['estado', 'LOC', 2121, 2127]\n",
      "['Sao Paulo', 'LOC', 2131, 2140]\n",
      "['lista', 'MISC', 2160, 2165]\n",
      "['clientes', 'PER', 2169, 2177]\n",
      "['espera', 'MISC', 2181, 2187]\n",
      "['seis millones', 'MISC', 2208, 2221]\n",
      "['personas', 'PER', 2225, 2233]\n",
      "['entrada', 'MISC', 2258, 2265]\n",
      "['Telefónica', 'ORG', 2269, 2279]\n",
      "['consejero delegado', 'PER', 2285, 2303]\n",
      "['Telefónica Internacional', 'ORG', 2307, 2331]\n",
      "['multinacional', 'ORG', 2353, 2366]\n",
      "['española', 'LOC', 2367, 2375]\n",
      "['anticipación', 'MISC', 2396, 2408]\n",
      "['metas', 'MISC', 2419, 2424]\n",
      "['ANATEL', 'ORG', 2440, 2446]\n",
      "['directivos', 'PER', 2453, 2463]\n",
      "['compañía', 'ORG', 2470, 2478]\n",
      "['nivel', 'MISC', 2509, 2514]\n",
      "['2002', 'MISC', 2545, 2549]\n",
      "['2002', 'MISC', 2545, 2549]\n",
      "['sector', 'MISC', 2576, 2582]\n",
      "['telecomunicaciones', 'MISC', 2590, 2608]\n",
      "['Brasil', 'LOC', 2612, 2618]\n",
      "['Telefónica', 'ORG', 2621, 2631]\n",
      "['operadora', 'PER', 2644, 2653]\n",
      "['telefonía', 'MISC', 2657, 2666]\n",
      "['Brasil', 'LOC', 2670, 2676]\n",
      "['Telesp', 'ORG', 2696, 2702]\n",
      "['Sao Paulo', 'LOC', 2708, 2717]\n",
      "['control', 'MISC', 2729, 2736]\n",
      "['Tele Sudeste Celular', 'LOC', 2752, 2772]\n",
      "['telefonía móvil', 'MISC', 2788, 2803]\n",
      "['estados', 'LOC', 2811, 2818]\n",
      "['Río de Janeiro', 'LOC', 2822, 2836]\n",
      "['Espíritu Santo', 'LOC', 2839, 2853]\n",
      "['Compañía Riograndense de Telecomunicaciones', 'ORG', 2864, 2907]\n",
      "['CRT', 'ORG', 2910, 2913]\n",
      "['Río Grande do Sul', 'LOC', 2921, 2938]\n",
      "['venta', 'MISC', 2946, 2951]\n",
      "['grupo', 'PER', 2960, 2965]\n",
      "['Brasil Telecom', 'ORG', 2966, 2980]\n",
      "['empresa', 'ORG', 3013, 3020]\n",
      "['participación', 'MISC', 3035, 3048]\n",
      "['Tele Leste Celular', 'ORG', 3052, 3070]\n",
      "['operadora', 'PER', 3073, 3082]\n",
      "['estados', 'LOC', 3096, 3103]\n",
      "['Bahía', 'LOC', 3107, 3112]\n",
      "['Sergipe', 'LOC', 3115, 3122]\n",
      "['española', 'LOC', 3150, 3158]\n",
      "['Iberdrola', 'ORG', 3159, 3168]\n",
      "['socia', 'PER', 3183, 3188]\n",
      "['Portugal Telecom', 'ORG', 3192, 3208]\n",
      "['Telesp Celular', 'ORG', 3212, 3226]\n",
      "['operadora', 'PER', 3232, 3241]\n",
      "['Sao Paulo', 'LOC', 3251, 3260]\n",
      "['trabajo', 'MISC', 3292, 3299]\n",
      "['cien', 'MISC', 3339, 3343]\n",
      "['ciento', 'MISC', 3348, 3354]\n",
      "['acciones', 'MISC', 3362, 3370]\n",
      "['empresas', 'ORG', 3378, 3386]\n",
      "['potencial', 'MISC', 3422, 3431]\n",
      "['Brasil', 'LOC', 3442, 3448]\n",
      "['particular', 'MISC', 3456, 3466]\n",
      "['sector', 'MISC', 3472, 3478]\n",
      "['telecomunicaciones', 'MISC', 3482, 3500]\n",
      "['Viana Baptista', 'PER', 3512, 3526]\n",
      "['operación', 'MISC', 3533, 3542]\n",
      "['compra', 'MISC', 3546, 3552]\n",
      "['lanzamiento', 'MISC', 3573, 3584]\n",
      "['una', 'MISC', 3588, 3591]\n",
      "['Oferta Pública de Acciones', 'MISC', 3592, 3618]\n",
      "['OPA', 'ORG', 3621, 3624]\n",
      "['Telesp', 'ORG', 3643, 3649]\n",
      "['Tele Sudeste Celular', 'ORG', 3655, 3675]\n",
      "['filiales', 'PER', 3692, 3700]\n",
      "['grupo', 'ORG', 3705, 3710]\n",
      "['español', 'LOC', 3711, 3718]\n",
      "['Argentina', 'LOC', 3722, 3731]\n",
      "['Perú', 'LOC', 3734, 3738]\n",
      "['Viana Baptista', 'PER', 3747, 3761]\n",
      "['Telefónica', 'ORG', 3764, 3774]\n",
      "['autorización', 'MISC', 3788, 3800]\n",
      "['órganos', 'MISC', 3808, 3815]\n",
      "['Argentina', 'LOC', 3831, 3840]\n",
      "['Perú', 'LOC', 3843, 3847]\n",
      "['bolsa', 'MISC', 3858, 3863]\n",
      "['Nueva York', 'LOC', 3867, 3877]\n",
      "['acciones', 'MISC', 3904, 3912]\n",
      "['cuatro', 'MISC', 3920, 3926]\n",
      "['empresas', 'ORG', 3927, 3935]\n",
      "['American Depositary Receipts', 'MISC', 3941, 3969]\n",
      "[\"ADR's\", 'MISC', 3972, 3977]\n",
      "['visto', 'MISC', 3996, 4001]\n",
      "['autoridades', 'PER', 4015, 4026]\n",
      "['operación', 'MISC', 4061, 4070]\n",
      "['compra', 'MISC', 4074, 4080]\n",
      "['una', 'MISC', 4116, 4119]\n",
      "['reunión', 'MISC', 4120, 4127]\n",
      "['Comisión de Valores Mobiliarios', 'ORG', 4135, 4166]\n",
      "['CVM', 'ORG', 4169, 4172]\n",
      "['aprobación', 'MISC', 4198, 4208]\n",
      "['OPAS', 'ORG', 4226, 4230]\n",
      "['forma', 'MISC', 4234, 4239]\n",
      "['Argentina', 'LOC', 4254, 4263]\n",
      "['Brasil', 'LOC', 4266, 4272]\n",
      "['Perú', 'LOC', 4275, 4279]\n",
      "['Estados Unidos', 'LOC', 4282, 4296]\n"
     ]
    }
   ],
   "source": [
    "s = 'LOC\\nHola'\n",
    "print(s)\n",
    "print(s.split('\\n'))\n",
    "\n",
    "with open('conll/gc_pred_named_entities.pkl','rb') as f:\n",
    "    v = pickle.load(f)\n",
    "    \n",
    "for i in range(len(v[0])):\n",
    "    print(v[0][i])\n",
    "del(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "f1a2e8c9-20b8-49f8-861e-d31f961b3e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "CONLL - SPACY\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 404/404 [00:03<00:00, 103.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "\n",
      "CONLL - SPACY\n",
      "DETECTION:\n",
      "\t REC :  0.9315428202883862\n",
      "\t PREC :  0.8202247191011236\n",
      "\t F1 :  0.8723468507333908\n",
      "\t JAC :  0.7735950112858181\n",
      "CLASSIFICATION:\n",
      "\t Class  LOC\n",
      "\t\t REC :  0.8971304818624797\n",
      "\t\t PREC :  0.5672714823690517\n",
      "\t\t F1 :  0.6950503355704698\n",
      "\t\t JAC :  0.5326261652201865\n",
      "\t Class  MISC\n",
      "\t\t REC :  0.5309278350515464\n",
      "\t\t PREC :  0.519327731092437\n",
      "\t\t F1 :  0.5250637213254036\n",
      "\t\t JAC :  0.35599078341013823\n",
      "\t Class  ORG\n",
      "\t\t REC :  0.6400911161731208\n",
      "\t\t PREC :  0.872369782683684\n",
      "\t\t F1 :  0.7383941605839415\n",
      "\t\t JAC :  0.5852811849109003\n",
      "\t Class  PER\n",
      "\t\t REC :  0.9044766708701135\n",
      "\t\t PREC :  0.9183738796414853\n",
      "\t\t F1 :  0.9113722998729352\n",
      "\t\t JAC :  0.8371753720455208\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 404/404 [00:04<00:00, 97.09it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "\n",
      "CONLL - SPACY\n",
      "DETECTION:\n",
      "\t REC :  0.9061058344640435\n",
      "\t PREC :  0.9351369517728113\n",
      "\t F1 :  0.9203925243949501\n",
      "\t JAC :  0.8525251493642445\n",
      "CLASSIFICATION:\n",
      "\t Class  LOC\n",
      "\t\t REC :  0.9272523782876329\n",
      "\t\t PREC :  0.611439114391144\n",
      "\t\t F1 :  0.7369357349344007\n",
      "\t\t JAC :  0.5834507042253522\n",
      "\t Class  MISC\n",
      "\t\t REC :  0\n",
      "\t\t PREC :  0\n",
      "\t\t F1 :  0\n",
      "\t\t JAC :  0\n",
      "\t Class  ORG\n",
      "\t\t REC :  0.7136004514672686\n",
      "\t\t PREC :  0.9536199095022625\n",
      "\t\t F1 :  0.816333118140736\n",
      "\t\t JAC :  0.6896645759476411\n",
      "\t Class  PER\n",
      "\t\t REC :  0.9354417998043691\n",
      "\t\t PREC :  0.9449934123847167\n",
      "\t\t F1 :  0.9401933475339996\n",
      "\t\t JAC :  0.8871366728509585\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "\n",
      "==========\n",
      "CONLL - FLAIR\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 404/404 [00:03<00:00, 122.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "\n",
      "CONLL - FLAIR\n",
      "DETECTION:\n",
      "\t REC :  0.9746662413997357\n",
      "\t PREC :  0.9939593885042517\n",
      "\t F1 :  0.984218275513021\n",
      "\t JAC :  0.968926937536803\n",
      "CLASSIFICATION:\n",
      "\t Class  LOC\n",
      "\t\t REC :  0.9270227392913802\n",
      "\t\t PREC :  0.9558342420937841\n",
      "\t\t F1 :  0.9412080536912752\n",
      "\t\t JAC :  0.8889452332657201\n",
      "\t Class  MISC\n",
      "\t\t REC :  0.9559014267185474\n",
      "\t\t PREC :  0.9615133724722765\n",
      "\t\t F1 :  0.95869918699187\n",
      "\t\t JAC :  0.9206745783885072\n",
      "\t Class  ORG\n",
      "\t\t REC :  0.9706583969465649\n",
      "\t\t PREC :  0.9601226993865031\n",
      "\t\t F1 :  0.965361803084223\n",
      "\t\t JAC :  0.9330428800733777\n",
      "\t Class  PER\n",
      "\t\t REC :  0.9984457569163817\n",
      "\t\t PREC :  0.9922767995057151\n",
      "\t\t F1 :  0.9953517198636505\n",
      "\t\t JAC :  0.9907464528069093\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 404/404 [00:03<00:00, 114.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "\n",
      "CONLL - FLAIR\n",
      "DETECTION:\n",
      "\t REC :  0.9785015241456763\n",
      "\t PREC :  0.9898295915607249\n",
      "\t F1 :  0.984132960413081\n",
      "\t JAC :  0.968761582040557\n",
      "CLASSIFICATION:\n",
      "\t Class  LOC\n",
      "\t\t REC :  0.9309612320764737\n",
      "\t\t PREC :  0.9621295279912184\n",
      "\t\t F1 :  0.9462887989203778\n",
      "\t\t JAC :  0.8980532786885246\n",
      "\t Class  MISC\n",
      "\t\t REC :  0\n",
      "\t\t PREC :  0\n",
      "\t\t F1 :  0\n",
      "\t\t JAC :  0\n",
      "\t Class  ORG\n",
      "\t\t REC :  0.982612895435885\n",
      "\t\t PREC :  0.9697330791229742\n",
      "\t\t F1 :  0.9761305025788654\n",
      "\t\t JAC :  0.9533739456419869\n",
      "\t Class  PER\n",
      "\t\t REC :  0.9984457569163817\n",
      "\t\t PREC :  0.9965870307167235\n",
      "\t\t F1 :  0.9975155279503106\n",
      "\t\t JAC :  0.9950433705080545\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "\n",
      "==========\n",
      "CONLL - GC\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 404/404 [00:13<00:00, 29.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "\n",
      "CONLL - GC\n",
      "DETECTION:\n",
      "\t REC :  0.948727322579171\n",
      "\t PREC :  0.33204318272690925\n",
      "\t F1 :  0.49192019713771207\n",
      "\t JAC :  0.32618976542492184\n",
      "CLASSIFICATION:\n",
      "\t Class  LOC\n",
      "\t\t REC :  0.8677859391395593\n",
      "\t\t PREC :  0.6984797297297297\n",
      "\t\t F1 :  0.7739822180627048\n",
      "\t\t JAC :  0.6312977099236641\n",
      "\t Class  MISC\n",
      "\t\t REC :  0.5762081784386617\n",
      "\t\t PREC :  0.6373355263157895\n",
      "\t\t F1 :  0.605232331120656\n",
      "\t\t JAC :  0.4339305711086226\n",
      "\t Class  ORG\n",
      "\t\t REC :  0.7754247722235903\n",
      "\t\t PREC :  0.8838057816446815\n",
      "\t\t F1 :  0.8260755508919203\n",
      "\t\t JAC :  0.7036871508379888\n",
      "\t Class  PER\n",
      "\t\t REC :  0.9509742300439975\n",
      "\t\t PREC :  0.9040932178069914\n",
      "\t\t F1 :  0.9269413386429776\n",
      "\t\t JAC :  0.8638310019982872\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 404/404 [00:13<00:00, 29.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "\n",
      "CONLL - GC\n",
      "DETECTION:\n",
      "\t REC :  0.931185144729656\n",
      "\t PREC :  0.6248625668841163\n",
      "\t F1 :  0.7478726204052988\n",
      "\t JAC :  0.59728158060674\n",
      "CLASSIFICATION:\n",
      "\t Class  LOC\n",
      "\t\t REC :  0.8989130434782608\n",
      "\t\t PREC :  0.7525022747952684\n",
      "\t\t F1 :  0.8192174343734522\n",
      "\t\t JAC :  0.6937919463087249\n",
      "\t Class  MISC\n",
      "\t\t REC :  0\n",
      "\t\t PREC :  0\n",
      "\t\t F1 :  0\n",
      "\t\t JAC :  0\n",
      "\t Class  ORG\n",
      "\t\t REC :  0.8390620836664002\n",
      "\t\t PREC :  0.954531676265535\n",
      "\t\t F1 :  0.8930799773114011\n",
      "\t\t JAC :  0.8068152703048936\n",
      "\t Class  PER\n",
      "\t\t REC :  0.9714285714285714\n",
      "\t\t PREC :  0.942385549672999\n",
      "\t\t F1 :  0.9566866898514069\n",
      "\t\t JAC :  0.916969696969697\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "\n",
      "==========\n",
      "WIKINER - SPACY\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 286953/286953 [04:14<00:00, 1127.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "\n",
      "WIKINER - SPACY\n",
      "DETECTION:\n",
      "\t REC :  0.9821991503731712\n",
      "\t PREC :  0.9893720445522334\n",
      "\t F1 :  0.9857725493894102\n",
      "\t JAC :  0.9719442604279256\n",
      "CLASSIFICATION:\n",
      "\t Class  LOC\n",
      "\t\t REC :  0.9744091422341626\n",
      "\t\t PREC :  0.967272200212581\n",
      "\t\t F1 :  0.9708275547710719\n",
      "\t\t JAC :  0.9433089267708893\n",
      "\t Class  MISC\n",
      "\t\t REC :  0.9228210068143708\n",
      "\t\t PREC :  0.945247405350095\n",
      "\t\t F1 :  0.9338995902009279\n",
      "\t\t JAC :  0.8759959020904419\n",
      "\t Class  ORG\n",
      "\t\t REC :  0.9070502327302749\n",
      "\t\t PREC :  0.9100165657488861\n",
      "\t\t F1 :  0.9085309779935413\n",
      "\t\t JAC :  0.8323928207539776\n",
      "\t Class  PER\n",
      "\t\t REC :  0.9802157050308506\n",
      "\t\t PREC :  0.9774903478394531\n",
      "\t\t F1 :  0.9788511294261986\n",
      "\t\t JAC :  0.9585782814176398\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 286953/286953 [04:23<00:00, 1089.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "\n",
      "WIKINER - SPACY\n",
      "DETECTION:\n",
      "\t REC :  0.9825474863486339\n",
      "\t PREC :  0.9881902737443823\n",
      "\t F1 :  0.9853608015872024\n",
      "\t JAC :  0.9711440314237858\n",
      "CLASSIFICATION:\n",
      "\t Class  LOC\n",
      "\t\t REC :  0.9805495876133818\n",
      "\t\t PREC :  0.9805079682250497\n",
      "\t\t F1 :  0.980528777477573\n",
      "\t\t JAC :  0.9618013297633841\n",
      "\t Class  MISC\n",
      "\t\t REC :  0\n",
      "\t\t PREC :  0\n",
      "\t\t F1 :  0\n",
      "\t\t JAC :  0\n",
      "\t Class  ORG\n",
      "\t\t REC :  0.9434294681985076\n",
      "\t\t PREC :  0.9414780450327995\n",
      "\t\t F1 :  0.9424527464726241\n",
      "\t\t JAC :  0.8911684497587581\n",
      "\t Class  PER\n",
      "\t\t REC :  0.9862698141179154\n",
      "\t\t PREC :  0.9869586254747298\n",
      "\t\t F1 :  0.9866140995717516\n",
      "\t\t JAC :  0.973581830134815\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "\n",
      "==========\n",
      "WIKINER - FLAIR\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 286953/286953 [04:16<00:00, 1119.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "\n",
      "WIKINER - FLAIR\n",
      "DETECTION:\n",
      "\t REC :  0.9652009289489113\n",
      "\t PREC :  0.9752141993513983\n",
      "\t F1 :  0.9701817280332613\n",
      "\t JAC :  0.942090225472904\n",
      "CLASSIFICATION:\n",
      "\t Class  LOC\n",
      "\t\t REC :  0.8107732562188478\n",
      "\t\t PREC :  0.9604819636837929\n",
      "\t\t F1 :  0.8793008263125243\n",
      "\t\t JAC :  0.784600227212919\n",
      "\t Class  MISC\n",
      "\t\t REC :  0.8626786037258094\n",
      "\t\t PREC :  0.7153999025085305\n",
      "\t\t F1 :  0.7821666495849134\n",
      "\t\t JAC :  0.642260822729415\n",
      "\t Class  ORG\n",
      "\t\t REC :  0.8513139596564924\n",
      "\t\t PREC :  0.609557049735667\n",
      "\t\t F1 :  0.7104315470849335\n",
      "\t\t JAC :  0.5509064256953593\n",
      "\t Class  PER\n",
      "\t\t REC :  0.9521115660555113\n",
      "\t\t PREC :  0.9671276615322328\n",
      "\t\t F1 :  0.9595608709544895\n",
      "\t\t JAC :  0.9222652668155434\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 286953/286953 [04:27<00:00, 1072.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "\n",
      "WIKINER - FLAIR\n",
      "DETECTION:\n",
      "\t REC :  0.952834929159148\n",
      "\t PREC :  0.9864951596034719\n",
      "\t F1 :  0.9693729303363287\n",
      "\t JAC :  0.9405661454755577\n",
      "CLASSIFICATION:\n",
      "\t Class  LOC\n",
      "\t\t REC :  0.883126593892457\n",
      "\t\t PREC :  0.9798093146468634\n",
      "\t\t F1 :  0.9289591324552254\n",
      "\t\t JAC :  0.867342377499326\n",
      "\t Class  MISC\n",
      "\t\t REC :  0\n",
      "\t\t PREC :  0\n",
      "\t\t F1 :  0\n",
      "\t\t JAC :  0\n",
      "\t Class  ORG\n",
      "\t\t REC :  0.9507188830894361\n",
      "\t\t PREC :  0.6552637386890311\n",
      "\t\t F1 :  0.7758136374919975\n",
      "\t\t JAC :  0.633738180110568\n",
      "\t Class  PER\n",
      "\t\t REC :  0.9784810597950045\n",
      "\t\t PREC :  0.9823448868867092\n",
      "\t\t F1 :  0.9804091664859395\n",
      "\t\t JAC :  0.9615711854792967\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "\n",
      "==========\n",
      "WIKINER - GC\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 286953/286953 [17:07<00:00, 279.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "\n",
      "WIKINER - GC\n",
      "DETECTION:\n",
      "\t REC :  0.9902834343176575\n",
      "\t PREC :  0.325708107705915\n",
      "\t F1 :  0.49019060257507563\n",
      "\t JAC :  0.3246705202730403\n",
      "CLASSIFICATION:\n",
      "\t Class  LOC\n",
      "\t\t REC :  0.8726351232969224\n",
      "\t\t PREC :  0.9269059373276443\n",
      "\t\t F1 :  0.8989521768665066\n",
      "\t\t JAC :  0.8164515273352627\n",
      "\t Class  MISC\n",
      "\t\t REC :  0.7124752237371614\n",
      "\t\t PREC :  0.7554726594486398\n",
      "\t\t F1 :  0.7333442259541716\n",
      "\t\t JAC :  0.5789609465970339\n",
      "\t Class  ORG\n",
      "\t\t REC :  0.7908442597635961\n",
      "\t\t PREC :  0.5867923545648664\n",
      "\t\t F1 :  0.673706491906792\n",
      "\t\t JAC :  0.5079618408713843\n",
      "\t Class  PER\n",
      "\t\t REC :  0.9062095320890724\n",
      "\t\t PREC :  0.905217643014799\n",
      "\t\t F1 :  0.9057133159859713\n",
      "\t\t JAC :  0.827674620569869\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 286953/286953 [16:22<00:00, 292.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "\n",
      "WIKINER - GC\n",
      "DETECTION:\n",
      "\t REC :  0.9694178463210918\n",
      "\t PREC :  0.6013702856186159\n",
      "\t F1 :  0.7422758999407505\n",
      "\t JAC :  0.5901738703311665\n",
      "CLASSIFICATION:\n",
      "\t Class  LOC\n",
      "\t\t REC :  0.9052326887746336\n",
      "\t\t PREC :  0.9546336788918895\n",
      "\t\t F1 :  0.9292770996470554\n",
      "\t\t JAC :  0.8678969127686873\n",
      "\t Class  MISC\n",
      "\t\t REC :  0\n",
      "\t\t PREC :  0\n",
      "\t\t F1 :  0\n",
      "\t\t JAC :  0\n",
      "\t Class  ORG\n",
      "\t\t REC :  0.8680277349768876\n",
      "\t\t PREC :  0.7069534553942299\n",
      "\t\t F1 :  0.7792540131547097\n",
      "\t\t JAC :  0.6383424738249558\n",
      "\t Class  PER\n",
      "\t\t REC :  0.945722075479722\n",
      "\t\t PREC :  0.9418325043410335\n",
      "\t\t F1 :  0.9437732824067524\n",
      "\t\t JAC :  0.8935328624873881\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test_v0(corpus_pre, 'spacy', 'wikiner')\n",
    "\n",
    "for nombre_corpus in ['conll','wikiner']:\n",
    "    for model in ['spacy','flair','gc']:\n",
    "        print('='*10)\n",
    "        print(nombre_corpus.upper(), '-' , model.upper())\n",
    "        print('='*10)\n",
    "        \n",
    "        for clases_ign in [[], ['MISC']]:\n",
    "            suf = '_'+'_'.join(clases_ign) if len(clases_ign)>0 else ''\n",
    "            test_v1(true_named_entities = extract_true_named_entities(nombre_corpus)[:],\n",
    "                    pred_named_entities = predict_named_entities(model, nombre_corpus)[:],\n",
    "                    modelo = model, \n",
    "                    nombre_corpus = nombre_corpus,\n",
    "                    clases_ignoradas = clases_ign,\n",
    "                    sufijo_nombre_archivo = suf)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60efa862-3012-4e71-8732-861e85d1d83b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Metrics examples\n",
    "Named Entity Recognition aims to **detect** entities and **classify** them. So we have to measure both, detection and classification capacity.\n",
    "\n",
    "#### For _detection_ capacity it's enough to use the accuracy:\n",
    "$$ Accuracy = \\frac{TP+TN}{TP+TN + FP+FN} $$\n",
    "\n",
    "where \n",
    "- $ TP$ are the true positives entities detected (the model says that a **real** entity **is** an entity)\n",
    "- $ TN$ are the true negatives entities detected (the model says that a token that **a no real** entity **is not** an entity)\n",
    "- $ FP$ are the false positives entities detected (the model says that a **no real** entity **is** an entity)\n",
    "- $ FN$ are the false negatives entities detected (the model says that a **real** entity **is not** an entity)\n",
    "\n",
    "#### For the _classify_ task, some ideas could be: \n",
    "- Measure the accuracy on the entities that are detected\n",
    "- Measure the capacity of classify a specific class. For example, we want to study the 'PERS' class. We could use other metrics, like _recall_, _precision_ and _F1-Score_\n",
    "$$ recall = \\frac{TP}{TP+FN} $$\n",
    "$$ precision = \\frac{TP}{TP+FP} $$\n",
    "$$ f1-score = 2 \\frac{precision \\times recall}{precision+recall} $$\n",
    "    so, in this point, it's interesting to study each class with these metrics.\n",
    "    \n",
    "Here, _TP,TN,FP,FN_ refer to the specific entity class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1e2eaa-f68b-4f3b-bc20-15e775e0a5e7",
   "metadata": {},
   "source": [
    "## Testeo con CoNLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2394aa-01f8-47f2-8dc6-c5901ab21593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_num_original_entities_conll(corpus_pre):\n",
    "    num_original_entities = 0\n",
    "    for text in corpus_pre:\n",
    "        new_text = []\n",
    "        for token in text.split('\\n'):\n",
    "            ent_type = token.split(' ')[1]\n",
    "            \n",
    "            if ent_type.strip().upper() != 'O':\n",
    "                num_original_entities += 1\n",
    "        \n",
    "    return corpus\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7701fc1d-cd7e-4452-9d6a-c93914f8703e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean, 2486.8713540431586\n",
      "max, 8087\n",
      "min, 151\n",
      "<class 'list'>\n",
      "[[{'entity': 'B-LOC', 'score': 0.99986696, 'index': 7, 'word': 'Londres', 'start': None, 'end': None}], [{'entity': 'B-LOC', 'score': 0.99954164, 'index': 7, 'word': 'Madrid', 'start': None, 'end': None}, {'entity': 'B-ORG', 'score': 0.63106686, 'index': 9, 'word': 'Real', 'start': None, 'end': None}, {'entity': 'I-LOC', 'score': 0.8082197, 'index': 10, 'word': 'Madrid', 'start': None, 'end': None}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 4899.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de entidades:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "print(\"mean,\",np.array([len(n) for n in noticias]).mean())\n",
    "print(\"max,\",np.array([len(n) for n in noticias]).max())\n",
    "print(\"min,\",np.array([len(n) for n in noticias]).min())\n",
    "nlp_ner = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"mrm8488/bert-spanish-cased-finetuned-ner\",\n",
    "    tokenizer=(\n",
    "        'mrm8488/bert-spanish-cased-finetuned-ner',  \n",
    "        {\"use_fast\": False, \"model_max_length\":87})\n",
    ")\n",
    "\n",
    "text = ['Mis amigos están pensando viajar a Londres este verano','segunda frase del 2022 en Madrid o Real Madrid']\n",
    "\n",
    "entidades_transf = []\n",
    "\n",
    "output = nlp_ner(text)\n",
    "print(type(output))\n",
    "print(output)\n",
    "for new_list in tqdm(output):\n",
    "    for ent_dict in new_list:\n",
    "        entidades_transf.append(prepare_string(ent_dict['entity']))\n",
    "        \n",
    "\n",
    "entidades_transf = list(set(entidades_transf))\n",
    "print(\"Número de entidades: \",len(entidades_transf))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
